{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.applications import ResNet50\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('/Users/mahikanair/.pyenv/runs/classify/train3/weights/best.pt')  #pre-trained(by us) yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands #model trained to identify hands through landmarks \n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.3)\n",
    "#kind of hands is only images (static) with the confidence being the level of confidence that the object being detected is a hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_video(img):\n",
    "    #the image is currently a numpy array (uint.8), we need it in the jpg file format for yolo\n",
    "    cv2.imwrite('saved_image.jpg', cv2.cvtColor(img, cv2.COLOR_BGR2RGB)) #making sure it's saved as RGB\n",
    "    results = model('saved_image.jpg') #direct jpg to model for correct classification, results is a list\n",
    "    prediction = results[0].probs.top1 #results[0] gives the first object detected, each element of results is a module with classes in it\n",
    "    #the class probs has the instance variable top1 for the class number that has the highest prediction\n",
    "    return prediction #this is our predicted class for this image \n",
    "\n",
    "\n",
    "def box_maker(mhl, h, w):\n",
    "    x_land = []\n",
    "    y_land = []\n",
    "    for hand_landmarks in mhl: #there should be 21 landmarks for each hand \n",
    "        print(hand_landmarks)\n",
    "        for i in range(len(hand_landmarks.landmark)):\n",
    "            x = hand_landmarks.landmark[i].x\n",
    "            y = hand_landmarks.landmark[i].y\n",
    "            x_land.append(x)\n",
    "            y_land.append(y)\n",
    "        \n",
    "        #we found out these are normalised coordinates, so we had to multiply it with the dimensios of the image to get the real coordinates\n",
    "        x1 = int(min(x_land)*w) \n",
    "        y1 = int(min(y_land)*h) \n",
    "        \n",
    "        x2 = int(max(x_land)*w) \n",
    "        y2 = int(max(y_land)*h) \n",
    "        \n",
    "        \n",
    "        #a rectangular box is fine for display around the hand, but to avoid distortion we need to make square boxes around the ROI for prediction\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        longer_side = max(width, height)\n",
    "        new_x1 = x1 + (width - longer_side) // 2 - 20\n",
    "        new_y1 = y1 + (height - longer_side) // 2 - 20\n",
    "        new_x2 = new_x1 + longer_side + 20\n",
    "        new_y2 = new_y1 + longer_side + 20\n",
    "        #the -+20s are to make it slightly bigger and make sure the box isnt cutting off the fingers \n",
    "        #absolute to prevent negative values which could crash the model \n",
    "        new_x1 = abs(new_x1)\n",
    "        new_x2 = abs(new_x2)\n",
    "        new_y1 = abs(new_y1)\n",
    "        new_y2 = abs(new_y2)\n",
    "        \n",
    "    return x1, y1, x2, y2, new_x1, new_y1, new_x2, new_y2\n",
    "\n",
    "\n",
    "capture = cv2.VideoCapture(1)\n",
    "\n",
    "#label mappigs to mudra names \n",
    "labels_dict = {0: 'alapadma', 1: 'arala', 12: 'ardhachandra', 23: 'ardhapataka', 24: 'bhramhara', 25: 'chandrakala', 26: 'chatura', 27:'hamsapaksha', 28:'hamsasya', 29:'kangula', 2:'kapitha', 3:'kartarimukha', 4:'katakamukha-1', 5:'katakamukha-2', 6:'katakamukha-3', 7:'mayura', 8:'mrighashisha', 9:'mukula', 10:'mushthi', 11:'padmakosha', 13:'pataka', 14:'santamsha', 15:'sarpashisha', 16:'shikhara', 17:'shukatunda', 18:'singhamukha', 19:'suchi', 20:'tamrachuda', 21:'tripataka', 22:'trishula'}\n",
    "\n",
    "while True:\n",
    "    ret, frame = capture.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) #the image \n",
    "    h, w, _ = frame.shape\n",
    "    results = hands.process(frame) #calculates the hand landmarks \n",
    "    if results.multi_hand_landmarks: #if it found the landmarks \n",
    "        #we want to detect 1 or 2 hands (essentially for one dancer)\n",
    "        if len(results.multi_hand_landmarks) == 1:  \n",
    "            x1, y1, x2, y2, new_x1, new_y1, new_x2, new_y2 = box_maker(results.multi_hand_landmarks, h, w)\n",
    "            #cropping frame with square coordinates\n",
    "            cropped_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "            \n",
    "            prediction = predict_from_video(cropped_frame)\n",
    "            #finding the name of the predicted mudra\n",
    "            predicted_character = labels_dict[prediction]\n",
    "            #frame for hand\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "            #text display for the mudra name\n",
    "            cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_DUPLEX, 2, (0, 0, 0), 3) \n",
    "            \n",
    "            cv2.imshow('frame', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)) #to see the frame in the video \n",
    "            \n",
    "        elif len(results.multi_hand_landmarks) == 2: #everything repeated twice for two hands \n",
    "            x11, y11, x21, y21, new_x11, new_y11, new_x21, new_y21 = box_maker([results.multi_hand_landmarks[0]], h, w)\n",
    "            x12, y12, x22, y22, new_x12, new_y12, new_x22, new_y22 = box_maker([results.multi_hand_landmarks[1]], h, w)\n",
    "            \n",
    "            cropped_frame1 = frame[new_y11:new_y21, new_x11:new_x21]\n",
    "            cropped_frame2 = frame[new_y12:new_y22, new_x12:new_x22]\n",
    "            \n",
    "            prediction1 = predict_from_video(cropped_frame1)\n",
    "            prediction2 = predict_from_video(cropped_frame2)\n",
    "            \n",
    "            predicted_character1 = labels_dict[prediction1]\n",
    "            predicted_character2 = labels_dict[prediction2]\n",
    "            \n",
    "            cv2.rectangle(frame, (x11, y11), (x21, y21), (0, 0, 0), 4)\n",
    "            cv2.rectangle(frame, (x12, y12), (x22, y22), (0, 0, 0), 4)\n",
    "            \n",
    "            cv2.putText(frame, predicted_character1, (x11, y11 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3)\n",
    "            cv2.putText(frame, predicted_character2, (x12, y12 - 10), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 3)\n",
    "            \n",
    "            cv2.imshow('frame', cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "        next = cv2.waitKey(25) #every 25 ms\n",
    "        if next == 27:  #to stop with esc key \n",
    "            break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
